{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[1, 1], [2, 1], [3, 1], [4, 1]]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def load(start_idx, end_idx, num_rows, fp_csv, fp_txt):\n",
    "    if os.path.exists(fp_csv):\n",
    "        df = pd.read_csv(fp_csv).drop('Unnamed: 0', axis = 1)\n",
    "        df = df.iloc[start_idx:end_idx,:]\n",
    "        return df\n",
    "    \n",
    "    else:\n",
    "        train_text = open(fp_txt,\"r\")\n",
    "        text_list = train_text.readlines()\n",
    "        num_rows=num_rows ##########\n",
    "        df = pd.DataFrame(index=np.arange(num_rows),columns=['label','doc1_title','doc2_title','doc1_body','doc2_body'])\n",
    "        i=0\n",
    "        for text in text_list:\n",
    "            if i>=start_idx and i<=end_idx:\n",
    "                split_text = text.split('|')\n",
    "                df['label'][i-start_idx] = split_text[0]\n",
    "                df['doc1_title'][i-start_idx] = split_text[1]\n",
    "                df['doc2_title'][i-start_idx] = split_text[2]\n",
    "                df['doc1_body'][i-start_idx] = split_text[3]\n",
    "                df['doc2_body'][i-start_idx] = split_text[4]\n",
    "        i+=1\n",
    "        return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "136"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(embed_list[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('embed_2.pkl', 'rb') as f:\n",
    "    embed_list = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "import utils\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "df = utils.load(200,\"data/csv/100_train.csv\",\"data/text/100_train.txt\")\n",
    "\n",
    "df = df.iloc[:2,:]\n",
    "\n",
    "ex_tokens = [str(n)+\".\" for n in np.arange(50)] # tokens to be excluded\n",
    "ex_tokens+= [str(n)+\". \" for n in np.arange(50)]\n",
    "ex_tokens+= ['S.','S. ']\n",
    "\n",
    "split_sents = utils.split_into_sentences(df['doc1_body'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in C:\\Users\\yash/.cache\\torch\\hub\\pytorch_fairseq_master\n"
     ]
    }
   ],
   "source": [
    "import roberta \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(embd_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "#function to split text to sentences\n",
    "\n",
    "#input: text, list of tokens to be excluded\n",
    "#output: list of sentences\n",
    "\n",
    "import re\n",
    "alphabets= \"([A-Za-z])\"\n",
    "prefixes = \"(Mr|St|Mrs|Ms|Dr)[.]\"\n",
    "suffixes = \"(Inc|Ltd|Jr|Sr|Co)\"\n",
    "starters = \"(Mr|Mrs|Ms|Dr|He\\s|She\\s|It\\s|They\\s|Their\\s|Our\\s|We\\s|But\\s|However\\s|That\\s|This\\s|Wherever)\"\n",
    "acronyms = \"([A-Z][.][A-Z][.](?:[A-Z][.])?)\"\n",
    "websites = \"[.](com|net|org|io|gov)\"\n",
    "\n",
    "ex_tokens = [str(n)+\".\" for n in np.arange(50)] # tokens to be excluded\n",
    "ex_tokens+= [str(n)+\". \" for n in np.arange(50)]\n",
    "ex_tokens+=['S.','S. ']\n",
    "\n",
    "def split_into_sentences(text, ex_tokens):\n",
    "    text = \" \" + text + \"  \"\n",
    "    text = text.replace(\"\\n\",\" \")\n",
    "    text = re.sub(prefixes,\"\\\\1<prd>\",text)\n",
    "    text = re.sub(websites,\"<prd>\\\\1\",text)\n",
    "    if \"Ph.D\" in text: text = text.replace(\"Ph.D.\",\"Ph<prd>D<prd>\")\n",
    "    if \"No.\" in text: text = text.replace(\"No.\",\"No<prd>\")\n",
    "    if \"Rs.\" in text: text = text.replace(\"Rs.\",\"Rs<prd>\")\n",
    "    if \"s.\" in text: text = text.replace(\"s.\",\"s<prd>\")\n",
    "    if \"S.\" in text: text = text.replace(\"S.\",\"S<prd>\")\n",
    "    if \"cl.\" in text: text = text.replace(\"cl.\",\"cl<prd>\")\n",
    "    if \"Will.\" in text: text = text.replace(\"Will.\",\"Will<prd>\")\n",
    "    text = re.sub(\"\\s\" + alphabets + \"[.] \",\" \\\\1<prd> \",text)\n",
    "    text = re.sub(acronyms+\" \"+starters,\"\\\\1<stop> \\\\2\",text)\n",
    "    text = re.sub(alphabets + \"[.]\" + alphabets + \"[.]\" + alphabets + \"[.]\",\"\\\\1<prd>\\\\2<prd>\\\\3<prd>\",text)\n",
    "    text = re.sub(alphabets + \"[.]\" + alphabets + \"[.]\",\"\\\\1<prd>\\\\2<prd>\",text)\n",
    "    text = re.sub(\" \"+suffixes+\"[.] \"+starters,\" \\\\1<stop> \\\\2\",text)\n",
    "    text = re.sub(\" \"+suffixes+\"[.]\",\" \\\\1<prd>\",text)\n",
    "    text = re.sub(\" \" + alphabets + \"[.]\",\" \\\\1<prd>\",text)\n",
    "    if \"”\" in text: text = text.replace(\".”\",\"”.\")\n",
    "    if \"\\\"\" in text: text = text.replace(\".\\\"\",\"\\\".\")\n",
    "    if \"!\" in text: text = text.replace(\"!\\\"\",\"\\\"!\")\n",
    "    if \"?\" in text: text = text.replace(\"?\\\"\",\"\\\"?\")\n",
    "    text = text.replace(\".\",\".<stop>\")\n",
    "    text = text.replace(\"?\",\"?<stop>\")\n",
    "    text = text.replace(\"!\",\"!<stop>\")\n",
    "    text = text.replace(\"<prd>\",\".\")\n",
    "    sentences = text.split(\"<stop>\")\n",
    "    sentences = sentences[:-1]\n",
    "    sentences = [s.strip() for s in sentences]\n",
    "    sentences = split_into_sentences(df['doc1_body'][0])\n",
    "    split_sentences  = [s for s in sentences if s not in ex_tokens]\n",
    "    return split_sentences\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train_text = open(\"data/text_data/100_train.txt\",\"r\")\n",
    "text_list = train_text.readlines()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np \n",
    "\n",
    "num_rows=200 ##########\n",
    "df = pd.DataFrame(index=np.arange(num_rows),columns=['label','doc1_title','doc2_title','doc1_body','doc2_body'])\n",
    "i=0\n",
    "for text in text_list:\n",
    "    split_text = text.split('|')\n",
    "    df['label'][i] = split_text[0]\n",
    "    df['doc1_title'][i] = split_text[1]\n",
    "    df['doc2_title'][i] = split_text[2]\n",
    "    df['doc1_body'][i] = split_text[3]\n",
    "    df['doc2_body'][i] = split_text[4]\n",
    "    i+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('train.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
